///
/// \author Anastasiadis Petros (panastas@cslab.ece.ntua.gr)
///
/// \brief Some CUDA function calls with added error-checking
///

#include <cblas.h>

#include "CoCopeLia_subkernels.hpp"
#include "CoCopeLia_Model.hpp"
#include "gpu_utils.hpp"
#include "cpu_utils.hpp"

/* global variable declaration */
float * gpu_dA = NULL, *gpu_dB = NULL, *gpu_dC = NULL;
long long A_sz = -1, B_sz = -1, C_sz = -1;
short lock_A = 0, lock_B = 0, lock_C = 0;
CoCoModel_p glob_model;

void CoCopeLia_Sgemm(CBLAS_TRANSPOSE cpu_op_A,  CBLAS_TRANSPOSE cpu_op_B, size_t M, size_t N, size_t K, float alpha, float* A, size_t ldA, float* B, size_t ldB, float beta, float* C, size_t ldC, int* Tin, short dev_id){
	if(!A)error("CoCopeLia_Sgemm_Tile: A matrix unallocated");
	else if(!B)error("CoCopeLia_Sgemm_Tile: B matrix unallocated");
	else if(!C)error("CoCopeLia_Sgemm_Tile: C matrix unallocated");

	// FIXME: Only works for No TRANS
	if(cpu_op_A!= CblasNoTrans || cpu_op_B!= CblasNoTrans) error("CoCopeLia_Sgemm_Tile: Only implemented for NO TRANSA/B");

	// TODO: For now use only one device;
	int cur_id; cudaGetDevice(&cur_id);
	if ( cur_id != dev_id) printf("CoCopeLia_Sgemm_Tile: Device change initiated(%d->%d)\n",cur_id, dev_id);
	cudaSetDevice(dev_id);

	short A_loc, B_loc, C_loc; 
	double cpu_timer = csecond();

        cudaHostRegister(A,sizeof(float)*M*K,cudaHostRegisterPortable);
        cudaHostRegister(B,sizeof(float)*K*N,cudaHostRegisterPortable);
        cudaHostRegister(C,sizeof(float)*M*N,cudaHostRegisterPortable);

	A_loc = CoCopeLia_ptr_check_cuda_9_2(A,dev_id);
	B_loc = CoCopeLia_ptr_check_cuda_9_2(B,dev_id);
	C_loc = CoCopeLia_ptr_check_cuda_9_2(C,dev_id);	
	cudaGetLastError();

	cpu_timer = csecond() - cpu_timer; 
	//fprintf(stderr, "CoCopeLia cudaHostRegister: t_reg = %lf ms\n", cpu_timer*1000);
	
	size_t T;
	if (*Tin > 0){
		T = *Tin; 
		fprintf(stderr, "CoCopelia with input tile T = %zu\n", T);
	}
	else{
		cpu_timer = csecond(); 
		CoCoModel_p model = NULL;
		if(!glob_model) model = glob_model = CoCoModel_gemm_init( M, N, K, A_loc, B_loc, C_loc, dev_id, "Sgemm", 1);
		else if (A_sz != M*K || B_sz != K*N || C_sz != M*N) model = CoCoModel_gemm_init( M, N, K, A_loc, B_loc, C_loc, dev_id, "Sgemm", 1);
		else model = glob_model;
		T = CoCoModel_optimize3(model);
		cpu_timer = csecond() - cpu_timer; 
		//fprintf(stderr, "CoCopeLia predicted optimal tile T=%zu: t_pred = %lf ms\n", T , cpu_timer*1000);
		*Tin = T;
	}


	if (T > M || T > N || T > K) error("CoCopeLia_Sgemm_Tile: T greater than dim"); 


	cpu_timer = csecond();

	short local_A = 0, local_B = 0, local_C = 0;
	float *A_dev, *B_dev, *C_dev;

	if (A_loc) {
		if(!gpu_dA) {
			gpu_dA = (float*) gpu_malloc(M * K *sizeof(float)); 
			lock_A = 1; 
			A_dev = gpu_dA; 
			A_sz = M * K; 
		}
		else if (lock_A || A_sz < M * K){
			A_dev = (float*) gpu_malloc(M * K *sizeof(float)); 
			local_A = 1;
		}
		else {
			lock_A = 1; 
			A_dev = gpu_dA;
		}
	}
	else A_dev = A; 

	if (B_loc) {
		if(!gpu_dB) {
			gpu_dB = (float*) gpu_malloc(N * K *sizeof(float)); 
			lock_B = 1; 
			B_dev = gpu_dB; 
			B_sz = N * K; 
		}
		else if (lock_B || B_sz < N * K){
			B_dev = (float*) gpu_malloc(N * K *sizeof(float)); 
			local_B = 1;
		}
		else {
			lock_B = 1; 
			B_dev = gpu_dB;
		}
	}
	else B_dev = B; 

	if (C_loc) {
		if(!gpu_dC) {
			gpu_dC = (float*) gpu_malloc(N * M *sizeof(float)); 
			lock_C = 1; 
			C_dev = gpu_dC; 
			C_sz = N * M; 
		}
		else if (lock_C || C_sz < M * M){
			C_dev = (float*) gpu_malloc(N * M *sizeof(float)); 
			local_C = 1;
		}
		else {
			lock_C = 1; 
			C_dev = gpu_dC;
		}
	}
	else C_dev = C; 

	cpu_timer = csecond() - cpu_timer;
	cudaCheckErrors();
	//fprintf(stderr, "Allocation successful for (A_loc = %d, B_loc = %d C_loc = %d): t_alloc = %lf ms\n", A_loc, B_loc, C_loc, cpu_timer*1000);

	/// Generalize for not exact tiles
	size_t Nlast = N%T , Mlast = M%T, Klast= K%T; 
	size_t M_parts = M/T , N_parts = N/T, K_parts = K/T, current_ctr, ptr_offset;
	
	if (Mlast > T/2) M_parts++;
	else Mlast+=T;
	if (Nlast > T/2) N_parts++;
	else Nlast+=T;
	if (Klast > T/2) K_parts++;
	else Klast+=T;

	int kernel_num = M_parts*N_parts*K_parts;

	kernel3f_p kernels[kernel_num];

	size_t Tm = T,Tn = T, Tk = T;

	for (int mi = 0; mi < M_parts; mi++)
	{
		if ( mi == M_parts - 1) Tm = Mlast;
		else Tm = T; 
		for (int ni = 0 ; ni < N_parts; ni++){
			if ( ni == N_parts - 1) Tn = Nlast;
			else Tn = T; 
			for (int ki = 0; ki < K_parts; ki++){
        			if ( ki == K_parts - 1) Tk = Klast;
				else Tk = T; 
        			current_ctr = mi*N_parts*K_parts + ni*K_parts + ki; 
				kernels[current_ctr] = CoCopeLia_Sgemm_subkernel_init(M, N, K, Tm, Tn, Tk, A_loc, B_loc, C_loc, dev_id);
			}
    		}
	}
	fprintf(stderr, "Mlast = %d, Nlast = %d, Klast = %d, Kernels = %d\n", Mlast, Nlast, Klast, kernel_num);
	cudaCheckErrors();
	//fprintf(stderr, "Device buffer allocation(%d,%d,%d) successfully t = %lf ms ( %.3lf Gb/s)\n",ctrl->M , ctrl->N, ctrl->K, ctrl->alloc_t * 1000 ,1e-9 / (ctrl->alloc_t / (ctrl->M*ctrl->N + ctrl->M*ctrl->K + ctrl->N*ctrl->K) ));

cpu_timer = csecond();
for (int mi = 0; mi < M_parts; mi++)
  	for (int ni = 0 ; ni < N_parts; ni++)
	{
		for (int ki = 0; ki < K_parts; ki++)
		{
        		current_ctr = mi*N_parts*K_parts + ni*K_parts + ki; 
			if (current_ctr) {
				kernels[current_ctr]->prev_send = kernels[current_ctr-1]->data_avail;
				kernels[current_ctr]->prev_gemm = kernels[current_ctr-1]->gemm_complete;
			}
			else kernels[current_ctr]->prev_send = kernels[current_ctr]->prev_gemm = NULL;
        		ptr_offset = mi*T + ki*T*kernels[current_ctr]->ldA;
        		kernels[current_ctr]->A_dev = &A_dev[ptr_offset];
        		kernels[current_ctr]->As = &A[ptr_offset];

        		ptr_offset = ni*T*kernels[current_ctr]->ldB + T*ki;
        		kernels[current_ctr]->B_dev = &B_dev[ptr_offset];
        		kernels[current_ctr]->Bs = &B[ptr_offset];

        		ptr_offset = ni*T*kernels[current_ctr]->ldC + mi*T;
			kernels[current_ctr]->C_dev = &C_dev[ptr_offset];
			kernels[current_ctr]->Cs = &C[ptr_offset];
			short AT_master = 0 , BT_master = 0, CT_master = 0,  CT_out_master = 0; 
			if (!ni) AT_master = 1;
			if (!mi) BT_master = 1;
			if (!ki) CT_master = 1;
			if (ki == K_parts-1) CT_out_master = 1;
			if (!ki) CoCopeLia_Sgemm_subkernel_async(alpha, beta, kernels[current_ctr], AT_master, BT_master, CT_master, CT_out_master);
        		else CoCopeLia_Sgemm_subkernel_async(alpha, 1.0, kernels[current_ctr], AT_master, BT_master, CT_master, CT_out_master);
    		}
	}
cudaCheckErrors();
cpu_timer = csecond() - cpu_timer;
fprintf(stderr, "Kernel execution successful: t = %lf ms\n\n", cpu_timer*1000);
cpu_timer = csecond();
for (int mi = 0; mi < M_parts; mi++)
  for (int ni = 0 ; ni < N_parts; ni++)
	for (int ki = 0; ki < K_parts; ki++){
        current_ctr = mi*N_parts*K_parts + ni*K_parts + ki; 
	CoCopeLia_Sgemm_subkernel_destroy(kernels[current_ctr]);
}
cudaCheckErrors();
cudaHostUnregister(A);
cudaHostUnregister(B);
cudaHostUnregister(C);
cudaGetLastError();

if (local_A) vec_free((void**)&A_dev, dev_id);
else lock_A = 0; 
if (local_B) vec_free((void**)&B_dev, dev_id);	
else lock_B = 0; 
if (local_C) vec_free((void**)&C_dev, dev_id);
else lock_C = 0; 

cudaCheckErrors();
cpu_timer = csecond() - cpu_timer;
return;
}
